# INSTALLATION, Running, & General Notes
Here is how to install and run my code:
* Boot up the GitHub codesoace on the 24Aug14 branch (this is the only place where I ran or wrote code so there is no gurantee it will work as expected anywhere else)
* Ensure that you have set the variable python3 correctly as per the instructions on the course website within the codespace
* In the hw3 directory, there is a script called run.sh, all you need to do is run this script, you may need to enable executable permissions "chmod +x run.sh"
* That file will queue and run all the jobs needed for this test in the background so you can do other things while it runs (takes approximately 15-30 min)
* If you do not wish to wait that long you can run just the python code with the command "python3 hw3.py [INSERT_INPUT_FILES_AND_PATHS]
* You may queue up as many files as you please for this, note that for all tests to execute during normal execution at least two files will need to be run on the same job
* Within the code for hw3.py I have placed several additional test cases via assert statements and two test methods, these are run during normal execution (as they were a great help in debuggiung) and do not need to be called separately, they test all cases indicated in the instructions and a few extras where I had issues
* Additionally, the code will automatically redirect standard out so do not mess with that, all that needs to be done is run the shell script, all output will be placed as appropriate into the appropriate folders within the out/ folder, lg for large (6 or more x cols), and sm for small, this was to easily facilitate the use of rqsh when viewing between large and small dimensionality data

# Results
## High-dimensional Data
### Brief breakdown of normal table for high dimensionality
![lg_normal](https://github.com/jphulse/ezr/blob/24Aug14/hw3/hw3_lg_rqsh.png)
  In the table above there are sveral interesting features that I would like to point out.  The first, and I believe most interesting feature is the Rank statistic for the smart and dumb treatments, both of these achieve rank 0 88% of the time (smart) and 66% of the time (dumb).  While this does make smart a decisive victor over the random selection, it does beg the interesting question of whether the accuracy penalty is worth dealing with for the speed increase in certain scenarios where accuracy may not be as crucial as speed at all times.  The second table indicates the number of evals required to reach each rank, and as you can see from the table the random and active learner are not crazy-far off of each other in this category either, especially when you start to take in mind the + or - factor that is at play here.  The DELTAS I believe are a little less relevant in this experiment, but again both treatments are relatively close in ranks 1 and 2 which I think is the big takeaway here, which is surprising as we are dealing with high dimensionality.  
  
### Brief breakdown of divided table for high dimensionality
![lg_divided](https://github.com/jphulse/ezr/blob/24Aug14/hw3/hw3_lg_divided_rqsh.png)
  In the table above I have taken the same input used in the previous table and formatted it differently to more clearly break down the different sizes as different categories so they could be evaluated individually for a more complete study.  The only category I will be breaking down is the RANK section, as the EVAL section here doesn't tell us much, and the DELTA tells us the same things we found out in the above table.  The RANK, however reveals some interesting details, firstly the active learner is almost always better on high-dimensional data regardless of the size used over the random selection.  This is not particularly surprising to me, although what is surprising is how close these are with the class of smart/40 and dumb/50 actually both having the same percentage.  I also think that it is interesting to note the fact that for the random choice the percentage value strinctly increases with N, whereas the active learner does not see a monotonic increase, and actually decreases with 20 being the second highest based on RANK 0 percentage.  Just an interesting observation, I can't really say anything too definitive other than that it appears that the active learner may actually be harmed by gaining more information before it hits a certain point and starts to increase again as it refines it's model.

## Low-dimensional Data
### Brief breakdown of normal table for low dimensionality
![sm_normal](https://github.com/jphulse/ezr/blob/24Aug14/hw3/hw3_sm_rqsh.png)
  For the smaller data we observe similarly to the high-dimensional data that the "smart" treatment with the active learner generally outperforms the "dumb" treatment with random selection.  However, here that gap is a lot smaller, placing 80% in rank 0 (smart) and 73% (dumb), however smart also sinks down with 7% in rank 2 while 100% of dumb's are within ranks 0 and 1 indicating that there may be potential for the active learner to be worse given certain smaller datasets than random selection.  However, generally here in terms of rank I actually think that both are very close here, so there may not be a clear benefit to using the smart treatment on all low-dimensional data and there may be an argument for random choice to be used instead.  As for the EVALS those are pretty much where I would expect with the dumb treatment taking slightly more evaluations to get to its ranks than the smart treatment.  The deltas also demonstrate similar trends to the RANK category displaying the same trend of smart being above dumb in rank 0, below in rank 1, and the only one in rank 2.  Overall this data indicates a potential case to make for random guessing being a viable alternative treatment on, specifically prudent on low-dimensional data.
### Brief breakdown of divided table for low dimensionality
![sm_divided](https://github.com/jphulse/ezr/blob/24Aug14/hw3/hw3_sm_divided_rqsh.png)
  For the second table here nothing really sticks out as particularly crazy to me, I think the big thing is that we can observe a similar (but different) phenomenon where the active learner does not monotonically benefit from the increase in N for the smart treatment, while the dumb treatment either always improves or maintains when N increases in terms of rank.  The one thing that I will say though is that all of the smart choices besides smart/30 are tied with the best dumb choices (40 & 50) with 73% in rank 0, while smart/30 shoots up to 80%.  Overall this indicates to me that these are potentially interchangable in most cases for low-dimensionality data.

# Discussion and Conclusion
## JJR1
  Since I observed semi-contradictory data to this with smart/30 being the best treatment I would like to refine this hypothesis.  The new hypothesis I would propose would be more general.  I would assert that for low-dimensionality data it is suitable to use random selection with 40 or 50 random guesses or the active learners.  These can be considered largely equivalent based on the experiment I conducted and the results achieved, and this was ran on the vast majority of the datasets which makes this fairly thorough within the scope of this experiment.  So that is how I would refine this hypothesis.
## JJR2
  Since I observed that the random guessing was still not too far off of the learner I would like to refine this hypothesis too.  In the hypothesis it is asserted that the dumb treatment produces rubbish results, however based on my observations it is not too far off.   I'd be pretty hard pressed to call 66% rank 0 rubbish, so I would refine the hypothesis to say "in the case of higher-dimensionality data the active learner produces notably better results than random choice, but the random choice treatments still produce top ranked results within a surpisingly close margin to the learner.  This is especially surprising due to the increased complexity, so while I think at the larger sizes the active learner is a distinctively better choice generally, I do not believe that random choice produces "rubbish" as stated by the original hypothesis.
